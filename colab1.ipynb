{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14affd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab: Install all dependencies\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install diffusers==0.21.0 transformers==4.30.2 accelerate==0.20.3 safetensors==0.3.1 xformers==0.0.20 Pillow==9.5.0 numpy==1.24.4 matplotlib==3.7.2 gradio==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and warning suppression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"A matching Triton is not available\") \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import autocast\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from typing import Optional, Tuple, List\n",
    "from datetime import datetime\n",
    "from importlib.metadata import version\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    DDIMScheduler,\n",
    "    LMSDiscreteScheduler\n",
    ")\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Stable Diffusion Generator class\n",
    "class StableDiffusionGenerator:    \n",
    "    def __init__(self, model_id: str = \"runwayml/stable-diffusion-v1-5\", device: str = \"auto\"):\n",
    "        try:\n",
    "            self.device = self._setup_device(device)\n",
    "            self.dtype = torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
    "            \n",
    "            print(f\"üöÄ Initializing Stable Diffusion on {self.device}\")\n",
    "            print(f\"üìä Using precision: {self.dtype}\")\n",
    "            \n",
    "            torch_version = version(\"torch\")\n",
    "            diffusers_version = version(\"diffusers\")\n",
    "            print(f\"üì¶ PyTorch version: {torch_version}\")\n",
    "            print(f\"üì¶ Diffusers version: {diffusers_version}\")\n",
    "            \n",
    "            self.pipe = self._load_pipeline(model_id)\n",
    "            self.current_scheduler = \"euler_a\"\n",
    "            self.schedulers = {\n",
    "                \"euler_a\": (\"Euler Ancestral\", \"Fast, good for creative images\"),\n",
    "                \"euler\": (\"Euler\", \"Deterministic, consistent results\"),\n",
    "                \"ddim\": (\"DDIM\", \"Classic, good quality, slower\"),\n",
    "                \"dpm_solver\": (\"DPM Solver\", \"High quality, efficient\"),\n",
    "                \"lms\": (\"LMS\", \"Linear multistep, stable\")\n",
    "            }\n",
    "            print(\"‚úÖ Stable Diffusion Generator Ready!\")\n",
    "            print(f\"üìù Available Schedulers: {list(self.schedulers.keys())}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Initialization Error: {str(e)}\")\n",
    "            print(\"Please ensure Visual C++ Redistributable 2015-2022 is installed\")\n",
    "            raise\n",
    "    \n",
    "    def _setup_device(self, device: str) -> torch.device:\n",
    "        if device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "                print(f\"üéØ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                print(\"üíª Using CPU (GPU not available)\")\n",
    "        return torch.device(device)\n",
    "    \n",
    "    def _load_pipeline(self, model_id: str) -> StableDiffusionPipeline:\n",
    "        try:\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=self.dtype,\n",
    "                safety_checker=None,\n",
    "                requires_safety_checker=False,\n",
    "            )\n",
    "            print(\"üîß Applying Memory Optimizations...\")\n",
    "            pipe.enable_attention_slicing()\n",
    "            print(\"  ‚úì Attention Slicing: Enabled\")\n",
    "            pipe.enable_vae_slicing()\n",
    "            print(\"  ‚úì VAE Slicing: Enabled\")\n",
    "            try:\n",
    "                pipe.enable_xformers_memory_efficient_attention()\n",
    "                print(\"  ‚úì XFormers Attention: Enabled\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† XFormers: Not available ({e})\")\n",
    "            if self.device.type == \"cuda\":\n",
    "                try:\n",
    "                    pipe = pipe.to(self.device)\n",
    "                    print(\"  ‚úì Full GPU Loading: Success\")\n",
    "                except RuntimeError as e:\n",
    "                    print(\"  ‚ö† GPU Memory Limited: Using CPU Offload\")\n",
    "                    pipe.enable_model_cpu_offload()\n",
    "            else:\n",
    "                pipe.enable_sequential_cpu_offload()\n",
    "                print(\"  ‚úì CPU Sequential Offload: Enabled\")\n",
    "            return pipe\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {e}\")\n",
    "    \n",
    "    def set_scheduler(self, scheduler_name: str) -> bool:\n",
    "        if scheduler_name not in self.schedulers:\n",
    "            print(f\"‚ùå Unknown scheduler: {scheduler_name}\")\n",
    "            return False\n",
    "        if scheduler_name == self.current_scheduler:\n",
    "            return True\n",
    "        scheduler_map = {\n",
    "            \"euler_a\": EulerAncestralDiscreteScheduler,\n",
    "            \"euler\": EulerDiscreteScheduler,\n",
    "            \"ddim\": DDIMScheduler,\n",
    "            \"dpm_solver\": DPMSolverMultistepScheduler,\n",
    "            \"lms\": LMSDiscreteScheduler\n",
    "        }\n",
    "        try:\n",
    "            scheduler_class = scheduler_map[scheduler_name]\n",
    "            self.pipe.scheduler = scheduler_class.from_config(self.pipe.scheduler.config)\n",
    "            self.current_scheduler = scheduler_name\n",
    "            name, desc = self.schedulers[scheduler_name]\n",
    "            print(f\"üîÑ Scheduler Changed: {name} ({desc})\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Scheduler Error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_image(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        negative_prompt: str = \"\",\n",
    "        width: int = 512,\n",
    "        height: int = 512,\n",
    "        num_inference_steps: int = 20,\n",
    "        guidance_scale: float = 7.5,\n",
    "        seed: Optional[int] = None,\n",
    "        scheduler: str = \"euler_a\"\n",
    "    ) -> Tuple[Image.Image, dict]:        \n",
    "        if not prompt.strip():\n",
    "            raise ValueError(\"Prompt cannot be empty\")\n",
    "        self.set_scheduler(scheduler)\n",
    "        if seed is None:\n",
    "            seed = torch.randint(0, 2**32, (1,)).item()\n",
    "        generator = torch.Generator(device=self.device)\n",
    "        generator.manual_seed(seed)\n",
    "        width = (width // 8) * 8\n",
    "        height = (height // 8) * 8\n",
    "        print(f\"üé® Generating: '{prompt[:50]}...'\")\n",
    "        print(f\"üìè Size: {width}x{height}, Steps: {num_inference_steps}, CFG: {guidance_scale}\")\n",
    "        print(f\"üé≤ Seed: {seed}, Scheduler: {scheduler}\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                if self.device.type == \"cuda\" and self.dtype == torch.float16:\n",
    "                    with autocast(self.device.type):\n",
    "                        result = self.pipe(\n",
    "                            prompt=prompt,\n",
    "                            negative_prompt=negative_prompt if negative_prompt else None,\n",
    "                            width=width,\n",
    "                            height=height,\n",
    "                            num_inference_steps=num_inference_steps,\n",
    "                            guidance_scale=guidance_scale,\n",
    "                            generator=generator\n",
    "                        )\n",
    "                else:\n",
    "                    result = self.pipe(\n",
    "                        prompt=prompt,\n",
    "                        negative_prompt=negative_prompt if negative_prompt else None,\n",
    "                        width=width,\n",
    "                        height=height,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        generator=generator\n",
    "                    )\n",
    "            generation_time = time.time() - start_time\n",
    "            metadata = {\n",
    "                \"prompt\": prompt,\n",
    "                \"negative_prompt\": negative_prompt,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"steps\": num_inference_steps,\n",
    "                \"guidance_scale\": guidance_scale,\n",
    "                \"scheduler\": scheduler,\n",
    "                \"seed\": seed,\n",
    "                \"generation_time\": round(generation_time, 2),\n",
    "                \"device\": str(self.device),\n",
    "                \"dtype\": str(self.dtype)\n",
    "            }\n",
    "            print(f\"‚úÖ Generated in {generation_time:.2f}s\")\n",
    "            return result.images[0], metadata\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            self._cleanup_memory()\n",
    "            raise RuntimeError(\n",
    "                \"GPU Out of Memory! Try: reducing image size, fewer steps, \"\n",
    "                \"or use CPU mode. Current settings may be too demanding.\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Generation failed: {str(e)}\")\n",
    "        finally:\n",
    "            self._cleanup_memory()\n",
    "    \n",
    "    def _cleanup_memory(self):\n",
    "        gc.collect()\n",
    "        if self.device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def get_memory_usage(self) -> dict:\n",
    "        memory_info = {}\n",
    "        if self.device.type == \"cuda\":\n",
    "            memory_info = {\n",
    "                \"allocated_gb\": torch.cuda.memory_allocated() / 1024**3,\n",
    "                \"reserved_gb\": torch.cuda.memory_reserved() / 1024**3,\n",
    "                \"max_allocated_gb\": torch.cuda.max_memory_allocated() / 1024**3,\n",
    "                \"total_gb\": torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            }\n",
    "        else:\n",
    "            memory_info = {\"device\": \"cpu\", \"note\": \"CPU memory tracking not available\"}\n",
    "        return memory_info\n",
    "    \n",
    "    def save_image(self, image: Image.Image, metadata: dict, output_dir: str = \"outputs\") -> str:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"sd_gen_{timestamp}_s{metadata['seed']}_{metadata['width']}x{metadata['height']}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        image.save(filepath)\n",
    "        metadata_file = filepath.replace('.png', '_metadata.txt')\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            f.write(\"Stable Diffusion Generation Metadata\\n\")\n",
    "            f.write(\"=\" * 40 + \"\\n\")\n",
    "            for key, value in metadata.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        print(f\"üíæ Saved: {filepath}\")\n",
    "        return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc682d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI class for Stable Diffusion\n",
    "class StableDiffusionUI:\n",
    "    def __init__(self):\n",
    "        self.generator = None\n",
    "        self.gallery_images = []\n",
    "        self.generation_history = []\n",
    "    \n",
    "    def initialize_generator(self, model_choice: str, device_choice: str) -> str:\n",
    "        try:\n",
    "            model_map = {\n",
    "                \"Stable Diffusion 1.5 (Recommended)\": \"runwayml/stable-diffusion-v1-5\",\n",
    "                \"Stable Diffusion 2.1\": \"stabilityai/stable-diffusion-2-1\",\n",
    "                \"Realistic Vision (RealVisXL)\": \"SG161222/RealVisXL_V4.0\"\n",
    "            }\n",
    "            device_map = {\n",
    "                \"Auto (Recommended)\": \"auto\",\n",
    "                \"GPU (CUDA)\": \"cuda\", \n",
    "                \"CPU (Slower)\": \"cpu\"\n",
    "            }\n",
    "            model_id = model_map.get(model_choice, \"runwayml/stable-diffusion-v1-5\")\n",
    "            device = device_map.get(device_choice, \"auto\")\n",
    "            self.generator = StableDiffusionGenerator(model_id=model_id, device=device)\n",
    "            memory_info = self.generator.get_memory_usage()\n",
    "            memory_text = f\"Memory Usage: {memory_info}\" if memory_info else \"Ready!\"\n",
    "            return f\"‚úÖ Model loaded successfully!\\n{memory_text}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Initialization failed: {str(e)}\"\n",
    "    \n",
    "    def generate_image(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        negative_prompt: str,\n",
    "        width: int,\n",
    "        height: int,\n",
    "        steps: int,\n",
    "        guidance: float,\n",
    "        scheduler: str,\n",
    "        seed: int,\n",
    "        save_image: bool\n",
    "    ) -> Tuple[Optional[Image.Image], str, str]:\n",
    "        if self.generator is None:\n",
    "            return None, \"‚ùå Please initialize the model first!\", \"\"\n",
    "        if not prompt.strip():\n",
    "            return None, \"‚ùå Please enter a prompt!\", \"\"\n",
    "        try:\n",
    "            seed = None if seed == -1 else int(seed)\n",
    "            image, metadata = self.generator.generate_image(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                num_inference_steps=steps,\n",
    "                guidance_scale=guidance,\n",
    "                scheduler=scheduler,\n",
    "                seed=seed\n",
    "            )\n",
    "            info_text = self._format_generation_info(metadata)\n",
    "            saved_path = \"\"\n",
    "            if save_image:\n",
    "                saved_path = self.generator.save_image(image, metadata)\n",
    "            self.generation_history.append(metadata)\n",
    "            self.gallery_images.append(image)\n",
    "            if len(self.gallery_images) > 10:\n",
    "                self.gallery_images = self.gallery_images[-10:]\n",
    "                self.generation_history = self.generation_history[-10:]\n",
    "            return image, info_text, saved_path\n",
    "        except Exception as e:\n",
    "            return None, f\"‚ùå Generation failed: {str(e)}\", \"\"\n",
    "    \n",
    "    def _format_generation_info(self, metadata: dict) -> str:\n",
    "        return f\"\"\"\n",
    "‚úÖ Generation Complete!\n",
    "\n",
    "üéØ **Parameters Used:**\n",
    "‚Ä¢ Prompt: {metadata['prompt'][:100]}{'...' if len(metadata['prompt']) > 100 else ''}\n",
    "‚Ä¢ Size: {metadata['width']} √ó {metadata['height']} pixels\n",
    "‚Ä¢ Steps: {metadata['steps']} (more steps = higher quality, slower)\n",
    "‚Ä¢ Guidance Scale: {metadata['guidance_scale']} (higher = follows prompt more closely)\n",
    "‚Ä¢ Scheduler: {metadata['scheduler']} \n",
    "‚Ä¢ Seed: {metadata['seed']} (for reproducible results)\n",
    "\n",
    "‚è±Ô∏è **Performance:**\n",
    "‚Ä¢ Generation Time: {metadata['generation_time']}s\n",
    "‚Ä¢ Device: {metadata['device']}\n",
    "‚Ä¢ Precision: {metadata['dtype']}\n",
    "\"\"\"\n",
    "    \n",
    "    def get_example_prompts(self) -> list:\n",
    "        return [\n",
    "            [\"a serene mountain landscape at sunrise, photorealistic, highly detailed\", \"blurry, low quality\"],\n",
    "            [\"portrait of a wise old wizard, fantasy art, digital painting\", \"ugly, deformed\"],\n",
    "            [\"cyberpunk cityscape at night, neon lights, futuristic\", \"daytime, bright\"],\n",
    "            [\"cute cartoon cat wearing a hat, kawaii style\", \"realistic, scary\"],\n",
    "            [\"abstract geometric patterns, colorful, modern art\", \"representational, dull colors\"]\n",
    "        ]\n",
    "    \n",
    "    def show_scheduler_info(self, scheduler: str) -> str:\n",
    "        scheduler_info = {\n",
    "            \"euler_a\": \"**Euler Ancestral**: Fast and creative, adds slight randomness for variety\",\n",
    "            \"euler\": \"**Euler**: Deterministic and consistent, same seed = same result\", \n",
    "            \"ddim\": \"**DDIM**: Classic scheduler, high quality but slower\",\n",
    "            \"dpm_solver\": \"**DPM Solver**: Efficient high-quality generation\",\n",
    "            \"lms\": \"**LMS**: Linear multistep, very stable results\"\n",
    "        }\n",
    "        return scheduler_info.get(scheduler, \"Scheduler information not available\")\n",
    "    \n",
    "    def get_memory_info(self) -> str:\n",
    "        if self.generator is None:\n",
    "            return \"Model not loaded\"\n",
    "        try:\n",
    "            memory_info = self.generator.get_memory_usage()\n",
    "            if 'allocated_gb' in memory_info:\n",
    "                return f\"\"\"\n",
    "GPU Memory Usage:\n",
    "‚Ä¢ Allocated: {memory_info['allocated_gb']:.2f}GB\n",
    "‚Ä¢ Reserved: {memory_info['reserved_gb']:.2f}GB  \n",
    "‚Ä¢ Total Available: {memory_info['total_gb']:.2f}GB\n",
    "‚Ä¢ Usage: {(memory_info['allocated_gb']/memory_info['total_gb']*100):.1f}%\n",
    "                \"\"\"\n",
    "            else:\n",
    "                return \"CPU mode - memory tracking not available\"\n",
    "        except:\n",
    "            return \"Memory info unavailable\"\n",
    "    \n",
    "    def create_interface(self) -> gr.Blocks:\n",
    "        with gr.Blocks(\n",
    "            title=\"üé® Educational Stable Diffusion Generator\",\n",
    "            theme=gr.themes.Soft()\n",
    "        ) as interface:\n",
    "            gr.Markdown(\"\"\"\n",
    "            # üé® Educational Stable Diffusion Text-to-Image Generator\n",
    "            **Learn Generative AI concepts while creating images!**\n",
    "            \"\"\")\n",
    "            with gr.Tab(\"üöÄ Setup & Generation\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"### üîß Model Setup\")\n",
    "                        model_choice = gr.Dropdown(\n",
    "                            choices=[\n",
    "                                \"Stable Diffusion 1.5 (Recommended)\",\n",
    "                                \"Stable Diffusion 2.1\", \n",
    "                                \"Realistic Vision (RealVisXL)\"\n",
    "                            ],\n",
    "                            value=\"Stable Diffusion 1.5 (Recommended)\",\n",
    "                            label=\"Model Selection\"\n",
    "                        )\n",
    "                        device_choice = gr.Dropdown(\n",
    "                            choices=[\n",
    "                                \"Auto (Recommended)\",\n",
    "                                \"GPU (CUDA)\",\n",
    "                                \"CPU (Slower)\"\n",
    "                            ],\n",
    "                            value=\"Auto (Recommended)\", \n",
    "                            label=\"Device Selection\"\n",
    "                        )\n",
    "                        init_btn = gr.Button(\"üöÄ Initialize Model\", variant=\"primary\")\n",
    "                        init_status = gr.Textbox(\n",
    "                            label=\"Initialization Status\",\n",
    "                            placeholder=\"Click Initialize Model to start\",\n",
    "                            lines=3\n",
    "                        )\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"### üìä System Info\")\n",
    "                        memory_btn = gr.Button(\"üìä Check Memory Usage\")\n",
    "                        memory_info = gr.Textbox(\n",
    "                            label=\"Memory Information\",\n",
    "                            placeholder=\"Click to check memory usage\",\n",
    "                            lines=6\n",
    "                        )\n",
    "                gr.Markdown(\"### ‚ú® Image Generation\")\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        prompt = gr.Textbox(\n",
    "                            label=\"üéØ Prompt (Describe what you want)\",\n",
    "                            placeholder=\"a beautiful landscape painting, oil on canvas, detailed\",\n",
    "                            lines=3\n",
    "                        )\n",
    "                        negative_prompt = gr.Textbox(\n",
    "                            label=\"üö´ Negative Prompt (What to avoid)\",\n",
    "                            placeholder=\"blurry, low quality, bad anatomy\",\n",
    "                            lines=2\n",
    "                        )\n",
    "                        generate_btn = gr.Button(\"üé® Generate Image\", variant=\"primary\", size=\"lg\")\n",
    "                    with gr.Column():\n",
    "                        with gr.Accordion(\"üîß Advanced Settings\", open=True):\n",
    "                            with gr.Row():\n",
    "                                width = gr.Slider(256, 1024, 512, step=64, label=\"Width\")\n",
    "                                height = gr.Slider(256, 1024, 512, step=64, label=\"Height\")\n",
    "                            with gr.Row():\n",
    "                                steps = gr.Slider(10, 100, 20, step=1, label=\"Inference Steps\")\n",
    "                                guidance = gr.Slider(1.0, 20.0, 7.5, step=0.5, label=\"Guidance Scale\")\n",
    "                            scheduler = gr.Dropdown(\n",
    "                                choices=[\"euler_a\", \"euler\", \"ddim\", \"dpm_solver\", \"lms\"],\n",
    "                                value=\"euler_a\",\n",
    "                                label=\"Scheduler\"\n",
    "                            )\n",
    "                            scheduler_info = gr.Textbox(\n",
    "                                label=\"Scheduler Information\",\n",
    "                                interactive=False,\n",
    "                                lines=2\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                seed = gr.Number(-1, label=\"Seed\")\n",
    "                                save_image = gr.Checkbox(True, label=\"üíæ Save Generated Images\")\n",
    "                with gr.Row():\n",
    "                    output_image = gr.Image(label=\"üñºÔ∏è Generated Image\", type=\"pil\")\n",
    "                with gr.Row():\n",
    "                    generation_info = gr.Textbox(\n",
    "                        label=\"üìù Generation Information\",\n",
    "                        lines=10,\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    saved_path = gr.Textbox(\n",
    "                        label=\"üíæ Saved File Path\",\n",
    "                        interactive=False\n",
    "                    )\n",
    "            with gr.Tab(\"üìö Learning Resources\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ## üß† Understanding Stable Diffusion\n",
    "                ### What is Diffusion?\n",
    "                Diffusion models learn to gradually remove noise from random data.\n",
    "                ### Key Components:\n",
    "                **üéØ CLIP (Text Encoder)**\n",
    "                **üßÆ U-Net (Denoising Network)** \n",
    "                **üé® VAE (Variational Autoencoder)**\n",
    "                **‚öôÔ∏è Schedulers**\n",
    "                ### Parameter Guide:\n",
    "                **Steps (10-100)**: More steps = higher quality but slower generation\n",
    "                **Guidance Scale (1-20)**: Higher values make the AI follow your prompt more strictly\n",
    "                **Seed**: Controls randomness - same seed + settings = same image\n",
    "                **Resolution**: Higher resolution = more detail but needs more GPU memory\n",
    "                \"\"\")\n",
    "            with gr.Tab(\"üñºÔ∏è Examples & Gallery\"):\n",
    "                gr.Markdown(\"### üé® Example Prompts to Try\")\n",
    "                examples = gr.Examples(\n",
    "                    examples=self.get_example_prompts(),\n",
    "                    inputs=[prompt, negative_prompt],\n",
    "                    label=\"Click any example to load it\"\n",
    "                )\n",
    "                gr.Markdown(\"### üñºÔ∏è Recent Generations\")\n",
    "                gallery = gr.Gallery(\n",
    "                    value=[],\n",
    "                    label=\"Your Generated Images\",\n",
    "                    show_label=True,\n",
    "                    elem_id=\"gallery\",\n",
    "                    columns=3,\n",
    "                    rows=2,\n",
    "                    object_fit=\"contain\",\n",
    "                    height=\"auto\"\n",
    "                )\n",
    "            # Event handlers\n",
    "            init_btn.click(\n",
    "                fn=self.initialize_generator,\n",
    "                inputs=[model_choice, device_choice],\n",
    "                outputs=init_status\n",
    "            )\n",
    "            generate_btn.click(\n",
    "                fn=self.generate_image,\n",
    "                inputs=[prompt, negative_prompt, width, height, steps, guidance, scheduler, seed, save_image],\n",
    "                outputs=[output_image, generation_info, saved_path]\n",
    "            ).then(\n",
    "                fn=lambda: self.gallery_images,\n",
    "                outputs=gallery\n",
    "            )\n",
    "            scheduler.change(\n",
    "                fn=self.show_scheduler_info,\n",
    "                inputs=scheduler,\n",
    "                outputs=scheduler_info\n",
    "            )\n",
    "            memory_btn.click(\n",
    "                fn=self.get_memory_info,\n",
    "                outputs=memory_info\n",
    "            )\n",
    "        return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio interface\n",
    "ui = StableDiffusionUI()\n",
    "interface = ui.create_interface()\n",
    "interface.launch(\n",
    "    share=True,  # Set to True for public sharing\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7860,\n",
    "    debug=False,\n",
    "    show_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393c665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09aa318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49aca57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710028ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
